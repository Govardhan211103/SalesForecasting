{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Walart Sales Forecasting\n",
    "## Exploratory Data Analysis\n",
    "### Problem:\n",
    "There are many seasons that sales are significantly higher or lower than averages. If the company does not know about these seasons, it can lose too much money. Predicting future sales is one of the most crucial plans for a company. Sales forecasting gives an idea to the company for arranging stocks, calculating revenue, and deciding to make a new investment. Another advantage of knowing future sales is that achieving predetermined targets from the beginning of the seasons can have a positive effect on stock prices and investors' perceptions. Also, not reaching the projected target could significantly damage stock prices, conversely. And, it will be a big problem especially for Walmart as a big company.\n",
    "\n",
    "## Aim:\n",
    "My aim in this project is to build a model which predicts sales of the stores. With this model, Walmart authorities can decide their future plans which is very important for arranging stocks, calculating revenue and deciding to make new investment or not.\n",
    "## Solution:\n",
    " With the accurate prediction company can;\n",
    "- Determine seasonal demands and take action for this\n",
    "- Protect from money loss because achieving sales targets can have a positive effect on stock prices and investors' perceptions\n",
    "- Forecast revenue easily and accurately\n",
    "- Manage inventories\n",
    "- Do more effective campaigns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset information(Feature description)\n",
    "- Store        : Store number\n",
    "- Dept         : Department number\n",
    "- Date         : Week\n",
    "- Weekly_Sales : Sales for given dept in given store\n",
    "- IsHoliday    : holiday or not\n",
    "- Temperature  : Average temperature in the region\n",
    "- Fuel_Price   : Cost of fuel in the region\n",
    "- MarkDown1 to 5: Anonymized data related to promotional markdowns that Walmart is running.\n",
    "- CPI          : Consumer price index\n",
    "- Unemployement: Unemployement in the region"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading the data from all the given Datasets.\n",
    "\n",
    "Sales_features=pd.read_csv('data/Sales_features.csv')\n",
    "Sales_stores=pd.read_csv('data/Sales_stores.csv')\n",
    "Sales_test=pd.read_csv('data/Sales_test.csv')\n",
    "Sales_train=pd.read_csv('data/Sales_train.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sales_featuers,Sales_stores and Sales_train contain some common features.\n",
    "# Need to be merged for cerating the training dataset.\n",
    "\n",
    "def showCols(data,name):\n",
    "    print(name,\" : \",data.columns)\n",
    "showCols(Sales_features,\"Sales_features\")\n",
    "showCols(Sales_train,\"Sales_train\")\n",
    "showCols(Sales_stores,\"Sales_stores\")\n",
    "showCols(Sales_test,\"Sales_test\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Datasets Sales_features and Sales_stores have 'Store' feature in common hence we need to merge the both datsets on Stores feature.<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset=Sales_features.merge(Sales_stores,how='inner',on='Store')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Firstly we need to change the date format into year and week\n",
    "\n",
    "dataset['Date']=pd.to_datetime(dataset['Date'])\n",
    "dataset['year']=dataset['Date'].dt.year ## Extracting 'year' data\n",
    "dataset['week']=dataset['Date'].apply(lambda x: datetime.strftime(x, '%U'))  ## Extracting 'week' data\n",
    "dataset.info()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the MarkDown1 to 5 features having about 58% null values which makes them to inappropriate for model training<br>\n",
    "Hence we drop those 5 features for better Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Droping the MarkDown1 to 5 features from  dataset\n",
    "\n",
    "dataset.drop(['MarkDown1','MarkDown2','MarkDown3','MarkDown4','MarkDown5'],axis=1,inplace=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training data\n",
    "Merging the 'dataset' and 'Sales_train' for making the training dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Sales_train['Date']=pd.to_datetime(Sales_train['Date'])\n",
    "\n",
    "df_train=Sales_train.merge(dataset,how='inner',on=['Store','IsHoliday','Date']) # merging Sales_train and dataset.\n",
    "\n",
    "# final Training Dataset\n",
    "df_train.head() "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Testing data\n",
    "Merging the 'dataset' and 'Sales_train' for making the Testing dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Sales_test['Date']=pd.to_datetime(Sales_test['Date'])\n",
    "df_test=Sales_test.merge(dataset,how='inner',on=['Store','IsHoliday','Date']) # merging Sales_test and dataset.\n",
    "\n",
    "# final Testing Dataset \n",
    "df_test.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "dropping the date column as we have both 'year' and 'week' features in both training and testing dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test.drop(['Date'],axis=1,inplace=True)\n",
    "df_train.drop(['Date'],axis=1,inplace=True)\n",
    "df_test.shape,df_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Converting the prepared df_train dataset into .csv file inorder to make the Dataingestion into the modules easier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to_csv file 'Sales_data'\n",
    "\n",
    "df_train.to_csv('data/Sales_data.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the indices of rows with null values for the specific feature\n",
    "null_ind = df_test[df_test['Unemployment'].isnull()].index\n",
    "\n",
    "# Delete the rows with null values\n",
    "df_test=df_test.drop(null_ind)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# looking at the target variable 'Weekly_Sales' it might contain outliers ,\n",
    "# Outliers in this data is having negative values in 'Weekly_Sales'\n",
    "\n",
    "df_train.loc[df_train['Weekly_Sales']<=0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dropping those outliers\n",
    "df_train=df_train.drop(df_train[df_train['Weekly_Sales']<=0].index,axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Categorical Variables -> 'Type', Percentage of stores in each type\n",
    "values=df_train['Type'].value_counts()\n",
    "plt.pie(values,labels=df_train['Type'].unique(),autopct='%.2f%%')\n",
    "plt.title(\"Percent of Store types\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Concluding that 'Type A' has greater no of stores comparing to others"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 'IsHoliday', Percentage of Holidays \n",
    "values=df_train['IsHoliday'].value_counts()\n",
    "plt.pie(values,labels=df_train['IsHoliday'].unique(),autopct='%.2f%%')\n",
    "plt.title(\"Percent of Holidays\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the no of Holidays too less"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see that the higher positive correlation found in betwee 'Fuel_Price' and 'year'.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Barplot bertween 'year' adn 'Fuel_Price'\n",
    "\n",
    "sns.barplot(x='year',y='Fuel_Price',data=df_train)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " FuelPrices are greatear in 2012 and 2011 when comapared to 2010 which is implying Positive correlation between them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Relationship of unemployement with the stores\n",
    "\n",
    "plt.rcParams['figure.figsize']=[10,3]\n",
    "sns.lineplot(data=df_train,y=df_train.Unemployment,x=df_train.Store) "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unemployment is almost common in every store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weekly_sales_2010=df_train[df_train['year']==2010]['Weekly_Sales'].groupby(df_train['week']).mean()\n",
    "weekly_sales_2011=df_train[df_train['year']==2011]['Weekly_Sales'].groupby(df_train['week']).mean()\n",
    "weekly_sales_2012=df_train[df_train['year']==2012]['Weekly_Sales'].groupby(df_train['week']).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Average Weekly_Sales per year \n",
    "\n",
    "fig,axs=plt.subplots(nrows=1,ncols=3,figsize=(15,3))\n",
    "plt.subplot(131)\n",
    "sns.lineplot(data=weekly_sales_2010,x= weekly_sales_2010.index,y=weekly_sales_2010.values)\n",
    "plt.subplot(132)\n",
    "sns.lineplot(data=weekly_sales_2011,x= weekly_sales_2011.index,y=weekly_sales_2011.values)\n",
    "plt.subplot(133)\n",
    "sns.lineplot(data=weekly_sales_2012,x= weekly_sales_2012.index,y=weekly_sales_2012.values)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Combining all of those weekly_sales in every year\n",
    "\n",
    "plt.rcParams['figure.figsize']=[15,5]\n",
    "sns.lineplot(data=weekly_sales_2010,x= weekly_sales_2010.index,y=weekly_sales_2010.values)\n",
    "sns.lineplot(data=weekly_sales_2011,x= weekly_sales_2011.index,y=weekly_sales_2011.values)\n",
    "sns.lineplot(data=weekly_sales_2012,x= weekly_sales_2012.index,y=weekly_sales_2012.values)\n",
    "plt.grid()\n",
    "plt.legend(['2010','2011','2012'],loc='best')\n",
    "plt.title(\"Average weekly sales for every year\")\n",
    "plt.xticks(np.arange(1,60))\n",
    "plt.show()  "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This concludes that in every year at the end of it the Weekly_Sales increases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Average sales in every department\n",
    "\n",
    "plt.rcParams['figure.figsize']=[20,5]\n",
    "sns.barplot(data=df_train,x='Dept',y='Weekly_Sales',palette='bright')\n",
    "plt.grid()\n",
    "plt.title(\"Average sales in every department\")\n",
    "plt.show()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Departments have various amounts of weekly_sales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Barplot between the 'Stores' and the 'Weekly_Sales', Average sales in each store\n",
    "\n",
    "plt.rcParams['figure.figsize']=[15,5]\n",
    "sns.barplot(x='Store',y='Weekly_Sales',data=df_train,palette='bright')\n",
    "plt.grid()\n",
    "plt.title(\"Average sales in every Store\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation between the features using confusion matrix\n",
    "\n",
    "sns.heatmap(data=df_train.corr(),annot=True,fmt='.2f',cmap='Blues')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Conclusion:\n",
    "\n",
    "- The 'Markdowns' are having almost 60% of Null values hence those are suggested to be dropped\n",
    "- The 'Date' feature is converted to 'year' and 'week' features as the problem stated that sales depend upon week highly\n",
    "- As observed that the 'Weekly_Sales' in every year are increasing at the end of the year\n",
    "- Holidays are too less comparing non-Holidays\n",
    "- 'Fuel_Price' is highly correlated with the 'year' feature\n",
    "- 'Size' and 'Dept' are noticably correalated with 'Weekly_Sales'"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
